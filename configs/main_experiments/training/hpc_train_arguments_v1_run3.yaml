data:
  # This is updated at runtime based on data version provided in CLI
  version: -1
  preprocessing: {}
  metadata: {}

train:
  model_name_or_path: 'google/t5-v1_1-base'
  max_seq_len: 1024 # maximum sequence length (decoder input)
  decoder_max_seq_len: 512
  epochs: 15 # maximum number of epochs
  data_size: -1 # number of examples in an epoch (-1: all examples available); use for testing
  batch_size: 16
  gradient_accumulation_steps: 2 # gradients applied every this many batches to the output
  use_scheduler: true
  warmup_steps: 1000 # for LR scheduling; scaled by gradient accumulation steps internally for Adafactor opt only; otherwise not
  learning_rate: 1e-4
  adam_eps: 1e-7
  optimizer: 'adafactor'
  relative_step: false # AdaFactor optimizer
  scale_parameter: false # AdaFactor optimizer
  fp16: false # use float16 in training
  eps: 1e-12
  # Path where checkpoints are *saved*
  checkpoint_dir: 'models'
  # If populated, the checkpoints are saved under checkpoint_dir/experiment_name
  experiment_name: 'seed_${..reproduce.seed}_d3st_v1_full'
  # Populate if the training is restarted from checkpoint
  checkpoint: ''
  verbose:
    disable_display: false
  data_parallel: false
  # Use this to save a model checkpoint after a specified number of batches
  global_step_checkpoints: []
  lr_log_freq: 50

dev:
  model_name_or_path: 'google/t5-v1_1-base'
  max_seq_len: 1024 # maximum sequence length
  decoder_max_seq_len: 512
  data_size: -1 # number of examples in an epoch (-1: all examples available); use for testing
  eval_interval: 160000  # number of examples after which the model is evaluated
  patience: 480000 # if dev JGA does not improve in this many examples after max, stop training
  batch_size: 32
  verbose:
    disable_display: false

decode:
  model_name_or_path: 'google/t5-v1_1-base'
  batch_size: 200
  max_seq_len: 1024 # maximum sequence length of inputs
  data_size: -1 # how many batches of examples to decode
  decode_only: [] # which dialogue IDs to decode
  # This field is only used if the decoding command is run with --all flag.
  decode_steps: []
  max_len: 1024 # maximum sequence length to be generated before <EOS>
  decoder_max_seq_len: 512
  temperature: 1.0
  num_beams: 1
  # Path where the hypothesis files are saved.
  # Will be suffixed by model binary file name (aka checkpoint.split("/")[-1]. Can be overridden via -hyp/--hyp_path.
  hyp_dir: 'hyps'
  metrics_dir: 'metrics'
  # Subdirectory in hyp_dir where files are to be saved
  # If set to `huggingface', calls Hugging Face API during decoding for generation. Might fail by predicting same
  # token repeatedly and failing to predict <EOS>. All subsequent calls to the API fail. If this happens set the
  # flag to `custom'
  generate_api: 'huggingface'
  # Maxinum number of tokens repeated consecutively. Only used when for generate_api is `custom'
  verbose:
    disable_display: false

reproduce:
  seed: 20220303 # same seed for random, NumPy, PyTorch (CPU/GPU), across devices
  cudnn:
    enabled: True
    deterministic: False
    benchmark: True
