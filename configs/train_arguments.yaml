data:
  # This is updated at runtime based on data version provided in CLI
  version: -1
  preprocessing: {}
  metadata: {}

train:
  model_name_or_path: 'google/t5-v1_1-base'
  max_seq_len: 1024 # maximum sequence length (decoder input)
  decoder_max_seq_len: 512
  epochs: 10 # maximum number of epochs
  data_size: -1 # number of examples in an epoch (-1: all examples available); use for testing
  batch_size: 4
  gradient_accumulation_steps: 8 # gradients applied every this many batches to the output
  max_grad_norm: 1.0
  use_scheduler: false
  warmup_steps: 0
  learning_rate: 1e-4
  adam_eps: 1e-7
  optimizer: 'adafactor'
  beta1: 0.0      # AdaFactor optimizer
  relative_step: false # AdaFactor optimizer
  fp16: false # use float16 in training
  eps: 1e-12
  # Path where checkpoints are *saved*
  checkpoint_dir: 'models'
  # If populated, the checkpoints are saved under checkpoint_dir/experiment_name
  experiment_name: 'default'
  # Populate if the training is restarted from checkpoint
  checkpoint: ''
  verbose:
    disable_display: false
  data_parallel: false
  # Use this to save a model checkpoint after a specified number of batches
  global_step_checkpoints: []

dev:
  model_name_or_path: 'google/t5-v1_1-base'
  max_seq_len: 1024 # maximum sequence length
  decoder_max_seq_len: 512
  data_size: -1 # number of examples in an epoch (-1: all examples available); use for testing
  eval_interval: 40000 # number of examples after which the model is evaluated
  batch_size: 8
  verbose:
    disable_display: false

reproduce:
  seed: 20220303 # same seed for random, NumPy, PyTorch (CPU/GPU), across devices
  cudnn:
    enabled: True
    deterministic: False
    benchmark: True
